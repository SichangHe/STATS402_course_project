\documentclass[a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
    \newcommand{\todo}[1]{\textcolor{red}{[ #1 ]}}
    \newcommand{\instruction}[1]{\textcolor{orange}{#1}}
    % \renewcommand{\instruction}[1]{} % Uncomment to hide instructions.

\newcommand{\hidden}[1]{}

\usepackage{biblatex}
    \addbibresource{references.bib}

\usepackage[colorlinks=false]{hyperref}

\title{STATS 402 - Interdisciplinary Data Analysis\\
    Resource-Constrained Deep Reinforcement Learning for Battlesnake\\
    Milestone Report: Stage 1
}
\author{Sichang He\\
    sichang.he@dukekunshan.edu.cn
}

\begin{document}
\maketitle

\subparagraph{Abstract}

\todo{Insert a very brief paragraph describing your project (200 words)}

\section{Project Rationale}

\todo{explain the motivation of the project
    selection by analyzing the characteristics of the selected data set,
    as well as the development trend of related fields, application prospects,
    and commonly used methods}

Battlesnake~\cite{battlesnake}
is a popular online programming competition in the form of a simultaneous
multiplayer board game. Similar to the arcade snake game,
each player controls a snake in real time on a finite grid board to be the last
one alive; the snake can change directions within each turn,
grow in length by eating randomly spawned food,
die from colliding with walls or snake bodies,
or starve to death after not eating for a long time (100 turns).
Instead of having human players control the snakes, in Battlesnake,
players develop a computer program to control their snakes' directions in each
turn, by implementing a web server that responds to the game server's request.
This means players can implement their algorithms freely,
as long as they can finish the response within the time limitation (500ms).

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{snake_game_screenshot.png}
    \caption{A Standard Battlesnake Game with 4 Snakes on A 11x11 Board.
        The transparent snake is already dead.
    }
    \label{fig:game}
\end{figure}

Battlesnake is an ideal target for implementing deep reinforcement learning.
Traditionally,
the leader board is occupied by players employing various heuristics and Monte
Carlo tree search algorithms,
most notably minimax~\cite{hill2018building,binnersley2020battlesnake}. However,
more recently,
even well-defined heuristics and tree search algorithms are only adequate for
intermediate-level gameplay~\cite{schier2019adversarial}. Additionally,
given the large search space (over $(3^n)^t$ raw possibilities for $t$ turns in
a game with $n$ snakes),
tree search algorithms are computationally demanding even when techniques like
alpha-beta pruning are employed. This cost problem is magnified,
especially considering the game sets a 500ms action time limit.
To address these problems, we compare Battlesnake to the game of Go,
which also has a large possibility space and potentially long time horizon,
and AlphaGo has demonstrated that deep reinforcement learning can excel in such
situations and achieve superhuman-level performance~\cite{silver2016mastering}.
More recent examples of deep reinforcement learning systems acing video games
include OpenAI Five defeating human world champions in Dota 2,
a much more complicated online multiplayer battle game~\cite{berner2019dota}.
Therefore,
it is sensible that deep reinforcement learning can possibly be employed to
build state-of-the-art Battlesnake agents.

A deeper aspect to explore is developing deep reinforcement learning agents to
operate in resource-constrained environments.
Most Battlesnake players are amateurs hosting their web servers on cheap VMs on
cloud platforms or Raspberry Pi
computers\footnote{\url{https://play.battlesnake.com/leaderboard/standard}.}.
Similar to these resource-constrained environments, use cases,
such as autonomous spacecraft control,
have seen deep reinforcement learning applied in
them~\cite{harris2022generation}. More closely related,
proximal policy optimization (PPO)~\cite{schulman2017proximal},
the same deep reinforcement learning technique employed by OpenAI Five,
has been successfully deployed in compute and memory-limited robotic control,
specifically quadrotor navigation~\cite{huang2023collision,hegde2023hyperppo}.
Thus,
developing a deep reinforcement learning Battlesnake agent capable of operating
on low-capability devices,
like low-tier cloud VMs and Raspberry Pis, should be feasible;
such solutions would also demonstrate more fairness and practical contributions
among the competition.

\section{Research Content and Objectives}

\todo{and critical scientific problems to be solved}

We propose to develop a purely deep-reinforcement-learning-based Battlesnake
agent capable of operating in resource-constrained environments. Specify,
we raise the following questions.

\paragraph{How well can a purely deep-reinforcement-learning-based agent
    perform?
} As shown by AlphaZero~\cite{silver2017mastering},
a deep reinforcement learning agent can learn,
with no other knowledge about the game than only a simulation environment,
is already enough to achieve superhuman performance in many board games.
However,
AlphaZero was built with a massive neural network and trained using 4000 TPUs,
a condition unthinkable for the ordinary Battlesnake players. Meanwhile,
prior Battlesnake implementations utilizing reinforcement learning often also
incorporate heuristics and tree search,
instead of relying purely on reinforcement learning
algorithms~\cite{chung2020battlesnake,binnersley2020battlesnake}. As such,
we are interested to learn how well such purely
deep-reinforcement-learning-based agents can perform.

\paragraph{How to strive a balance between fast inference time and high model
    capability?
}
We aim to develop our agent to be capable of operating on resource-constrained
environments,
akin to the low-tier VMs and Raspberry Pis other Battlesnake players deploy
their agents on. For our purpose,
we will target a standard VM provided by Duke University,
which has two Intel Xeon 6248R CPU cores and 3.6GiB of RAM;
this CPU is roughly twice as powerful as a Raspberry Pi 5's CPU\footnote{See
    \url{https://www.cpubenchmark.net/compare/3732vs5893/Intel-Xeon-Gold-6248R-vs-ARM-Cortex-A76-4-Core-2600-MHz}.
}. However, we also target a high performance in the game. Therefore,
we need to design the neural network of the agent,
such that the agent utilizes as much time as possible when performing inference.

\section{Research Plan and Feasibility Analysis}

\todo{including research methods, technical routes, experimental methods,
    key technologies, etc.}

The agent will be developed by referencing the framework
in~\cite{chung2020battlesnake}.

Feature extraction will follow the methods in~\cite{siddiqui2020multiagent}.
%TODO: Specify.

The reinforcement learning training will be done using
PPO~\cite{schulman2017proximal}.
We choose PPO because of its flexibility and high empirical performance in
adversarial games,
as observed
in~\cite{berner2019dota,binnersley2020battlesnake,chung2020battlesnake}.

We will first design a neural network that can reliably perform inference within
440ms on Duke's standard VM.
We choose to aim at 440ms because the average ping from the VM to the
Battlesnake website is around 20ms,
and we reserve 3 times the time for network out of the 500ms allocated action
time. We will train the agent a server with a GPU.

To evaluate our agent,
we propose to use the snake named ``ich heisse marvin'' as our benchmark target.
``ich heisse marvin'' is an open-source tree search Battlesnake implementation
that recently consistently ranks top 8 in the standard leaderboard while only
deployed on a Raspberry Pi; its strategy utilizes max\textsuperscript{n}
and alpha-beta pruning.
We plan to let different versions of our agent play against ``ich heisse
marvin'' and calculate the win rate,
so that we can evaluate its performance over time.

\section{Features, Innovations and the Expected Results}

\instruction{
    PENALTY FOR PLAGIARISM:\\
    \(\geq 30\%\): ZERO SCORE for the report.\\
    \((25\%,30\%\): -1.5 points\\
    \((20\%,\ 25\%\): -1.0 points\\
    (15\%, 20\%]: -0.5 points\\
    \(\leq 15\%\): no penalty, you will get 2 points
}

\printbibliography

\end{document}
