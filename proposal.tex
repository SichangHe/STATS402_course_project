\documentclass[a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xcolor}
    \newcommand{\todo}[1]{\textcolor{red}{[ #1 ]}}
    \newcommand{\instruction}[1]{\textcolor{orange}{#1}}
    % \renewcommand{\instruction}[1]{} % Uncomment to hide instructions.

\newcommand{\hidden}[1]{}

\usepackage{biblatex}
    \addbibresource{references.bib}

\usepackage[colorlinks=false]{hyperref}

\title{STATS 402 - Interdisciplinary Data Analysis\\
    Resource-Constrained Deep Reinforcement Learning for Battlesnake\\
    Milestone Report: Stage 1
}
\author{Sichang He\\
    sichang.he@dukekunshan.edu.cn
}

\begin{document}
\maketitle

\subparagraph{Abstract}

\todo{Insert a very brief paragraph describing your project (200 words)}

\section{Project Rationale}

\todo{explain the motivation of the project
    selection by analyzing the characteristics of the selected data set,
    as well as the development trend of related fields, application prospects,
    and commonly used methods}

Battlesnake~\cite{battlesnake}
is a popular online programming competition in the form of a simultaneous
multiplayer board game. Similar to the arcade snake game,
each player controls a snake in real time on a finite grid board to be the last
one alive; the snake can change directions within each turn,
grow in length by eating randomly spawned food,
die from colliding with walls or snake bodies,
or starve to death after not eating for a long time (100 turns),
as demonstrated in Fig.~\ref{fig:game}.
Instead of having human players control the snakes, in Battlesnake,
players develop a computer program to control their snakes' directions in each
turn, by implementing a web server that responds to the game server's request,
which contains the entire game state.
This means players can implement their algorithms freely,
as long as they can finish the response within the time limitation (500ms).

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{snake_game_screenshot.png}
    \caption{A Standard Battlesnake Game with 4 Snakes on A 11x11 Board.
        The transparent snake is already dead.
    }
    \label{fig:game}
\end{figure}

Battlesnake is an ideal target for implementing deep reinforcement learning.
Traditionally,
the leader board is occupied by players employing various heuristics and Monte
Carlo tree search algorithms,
most notably minimax~\cite{hill2018building,binnersley2020battlesnake}. However,
more recently,
even well-defined heuristics and tree search algorithms are only adequate for
intermediate-level gameplay~\cite{schier2019adversarial}. Additionally,
given the large search space (over $(3^n)^t$ raw possibilities for $t$ turns in
a game with $n$ snakes),
tree search algorithms are computationally demanding even when techniques like
alpha-beta pruning are employed. This cost problem is magnified,
especially considering the game sets a 500ms action time limit.
To address these problems, we compare Battlesnake to the game of Go,
which also has a large possibility space and potentially long time horizon,
and AlphaGo has demonstrated that deep reinforcement learning can excel in such
situations and achieve superhuman-level performance~\cite{silver2016mastering}.
More recent examples of deep reinforcement learning systems acing video games
include OpenAI Five defeating human world champions in Dota 2,
a much more complicated online multiplayer battle game~\cite{berner2019dota}.
Therefore,
it is sensible that deep reinforcement learning can possibly be employed to
build state-of-the-art Battlesnake agents.

A deeper aspect to explore is developing deep reinforcement learning agents to
operate in resource-constrained environments.
Most Battlesnake players are amateurs hosting their web servers on cheap VMs on
cloud platforms or Raspberry Pi
computers\footnote{\url{https://play.battlesnake.com/leaderboard/standard}.}.
Similar to these resource-constrained environments, use cases,
such as autonomous spacecraft control,
have seen deep reinforcement learning applied in
them~\cite{harris2022generation}. More closely related,
proximal policy optimization (PPO)~\cite{schulman2017proximal},
the same deep reinforcement learning technique employed by OpenAI Five,
has been successfully deployed in compute and memory-limited robotic control,
specifically quadrotor navigation~\cite{huang2023collision,hegde2023hyperppo}.
Thus,
developing a deep reinforcement learning Battlesnake agent capable of operating
on low-capability devices,
like low-tier cloud VMs and Raspberry Pis, should be feasible;
such solutions would also demonstrate more fairness and practical contributions
among the competition.

\section{Research Content and Objectives}

\todo{and critical scientific problems to be solved}

We propose to develop a purely deep-reinforcement-learning-based Battlesnake
agent capable of operating in resource-constrained environments. Specify,
we raise the following questions.

\paragraph{How well can a purely deep-reinforcement-learning-based agent
    perform?
} As shown by AlphaZero~\cite{silver2017mastering},
a deep reinforcement learning agent can learn,
with no other knowledge about the game than only a simulation environment,
is already enough to achieve superhuman performance in many board games.
However,
AlphaZero was built with a massive neural network and trained using 4000 TPUs,
a condition unthinkable for the ordinary Battlesnake players. Meanwhile,
prior Battlesnake implementations utilizing reinforcement learning often also
incorporate heuristics and tree search,
instead of relying purely on reinforcement learning
algorithms~\cite{chung2020battlesnake,binnersley2020battlesnake}. As such,
we are interested to learn how well such purely
deep-reinforcement-learning-based agents can perform.

\paragraph{How to strive a balance between fast inference time and high model
    capability?
}
We aim to develop our agent to be capable of operating on resource-constrained
environments,
akin to the low-tier VMs and Raspberry Pis other Battlesnake players deploy
their agents on. For our purpose,
we will target a standard VM provided by Duke University,
which has two Intel Xeon 6248R CPU cores and 3.6GiB of RAM;
this CPU is roughly twice as powerful as a Raspberry Pi 5's CPU\footnote{See
    \url{https://www.cpubenchmark.net/compare/3732vs5893/Intel-Xeon-Gold-6248R-vs-ARM-Cortex-A76-4-Core-2600-MHz}.
}. However, we also target a high performance in the game. Therefore,
we need to design the neural network of the agent,
such that the agent utilizes as much time as possible when performing inference.

\section{Research Plan and Feasibility Analysis}

\todo{including research methods, technical routes, experimental methods,
    key technologies, etc.}

The agent will be developed by referencing open-source implementations
in~\cite{siddiqui2020multiagent,chung2020battlesnake,wrenger2024rusty}.
It will be designed to compete in the standard Battlesnake game on an 11x11
board with 4 competing players.

Feature extraction will follow the basic methods
in~\cite{siddiqui2020multiagent,archinukmonte}, with some adjustments.
Specifically,
we will pad the board to a 21x21 grid and rotate it so that the snake's head is
centered and faces up. Therefore,
the three valid actions for the snake would be ``up'', ``left'',
and ``right'' only.
We will have 12 layers of 21x21 matrices ($\mathbb R^{12\times21\times21}$),
with each entry normalized to be within $(-1,1)$, and 0 indicating emptiness:
\begin{itemize}
    \item Wall layer. 1 indicates that the position is out of bound.
    \item Our snake's body layer.
        The value of each of our body segment position will be calculate as $$
            v_b:=\frac{2L_{\text{rest of body}}-1}{239}
        $$ where $L$ is the length of the snake.
        Large $v_b$'s indicate the position would remain occupied for longer.
        The tail always corresponds to a $v_b<0$ for distinction.
    \item Three opponent snakes' layers.
          These layers will be sorted by the opponents' length and health.
          Each opponent snake will be represented by 3 layers. \begin{itemize}
              \item Head layer. The value of each head will be calculated as $$
                        v_h:=\frac{1+2(L_{\text{opponent}}-L_{\text{us}})}{239}.
                    $$ $v_h>0$ indicates that the opponent snake is at least as
                    long as our snake,
                    so colliding with their head would kill our snake;
                    $v_h<0$ indicates the opposite.
              \item Body layer. This layer will also be calculated using $v_b$.
              \item Health layer.
                    The value will be
                    $\displaystyle\frac{H_{\text{opponent}}}{100}$ where $H$ is
                    the health of the snake.
                    This indicates how probable the opponent snake would survive
                    starvation.
          \end{itemize}
    \item Food layer.
          The value will be $\displaystyle\frac{101-H_{\text{us}}}{100}$ if
          there is food in the position.
          A large value indicates our snake is hungry.
\end{itemize}

The reinforcement learning training will be done using
PPO~\cite{schulman2017proximal}.
We choose PPO because of its flexibility and high empirical performance in
adversarial games,
as observed
in~\cite{berner2019dota,binnersley2020battlesnake,chung2020battlesnake}.
During training, we will simulate the standard game with four players.
To optimize our agent for competing against different levels of opponents,
each player of the simulated game will have a 20\% chance of being an older
version of our agent, similar to the setting in~\cite{silver2017mastering}.
However, to avoid useless simulations,
we will reassign the players if all of them are older versions.

We plan to develop our neural network gradually to fit the computation budget.
We will first use a simple multilayer perceptron (MLP)
to validate our PPO approach. Then,
we will design a neural network that can reliably perform inference within 440ms
on Duke's standard VM.
We choose to aim at 440ms because the average ping from the VM to the
Battlesnake website is around 20ms,
and we reserve 3 times the time for network out of the 500ms allocated action
time. We will train the agent a server with a GPU.

To benchmark our agent,
we propose to use the snake named ``ich heisse marvin'' as our comparison
target. \emph{ich heisse marvin}~\cite{wrenger2024rusty}
is an open-source tree search Battlesnake implementation.
It recently consistently ranks top 8 in the standard leaderboard,
despite being deployed on a Raspberry Pi. Its strategy utilizes
max\textsuperscript{n} and alpha-beta pruning.
We plan to let different versions of our agent play against \emph{ich heisse
    marvin} and calculate the win rate,
so that we can evaluate its performance over time.

\section{Features, Innovations and the Expected Results}

\instruction{
    PENALTY FOR PLAGIARISM:\\
    \(\geq 30\%\): ZERO SCORE for the report.\\
    \((25\%,30\%\): -1.5 points\\
    \((20\%,\ 25\%\): -1.0 points\\
    (15\%, 20\%]: -0.5 points\\
    \(\leq 15\%\): no penalty, you will get 2 points
}

\printbibliography

\end{document}
